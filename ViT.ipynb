{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08c8d39-47fd-48a5-9254-38da004ab409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b57bbfb2-ed75-454f-8e4b-322dce4f1dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.5,],std=[0.5,])])\n",
    "\n",
    "data_train=datasets.MNIST(root='D:/jupyter project/data/',transform=transform,train=True,download=False)\n",
    "data_test=datasets.MNIST(root='D:/jupyter project/data/',transform=transform,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a552777-81bb-4fe6-bbb2-19b74f856c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle = True to choose picture randomly\n",
    "data_loader_train=torch.utils.data.DataLoader(dataset=data_train,batch_size=640,shuffle=True)\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset =data_test,batch_size = 64,shuffle = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8721d5-93fa-49ec-a540-afbeabfc3c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 1, 28, 28])\n",
      "torch.Size([3, 2402, 242])\n",
      "after transpose shape: (2402, 242, 3)\n",
      "[tensor(5), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(1), tensor(2), tensor(1), tensor(8), tensor(2), tensor(8), tensor(7), tensor(9), tensor(3), tensor(3)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images,labels=next(iter(data_loader_train))\n",
    "# Splicing the 64 pictures into one picture to see what the pictures look like\n",
    "print(images.shape)\n",
    "img=torchvision.utils.make_grid(images)\n",
    "print(img.shape)\n",
    "# transpose the img because imshow need a img construction of (height,weight,channel) \n",
    "# but img has a construction of (channel,height,weight)\n",
    "\n",
    "img=img.numpy().transpose(1,2,0)\n",
    "\n",
    "print(\"after transpose shape:\",img.shape)\n",
    "\n",
    "print([labels[i].data for i in range(16)])\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d48ca75b-8e4c-4c5d-887f-4f21bb9b609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.net(input)\n",
    "        return output\n",
    " \n",
    "class MSA(nn.Module):\n",
    "    \"\"\"\n",
    "    dim就是输入的维度，也就是embeding的宽度\n",
    "    heads是有多少个patch\n",
    "    dim_head是每个patch要多少dim\n",
    "    dropout是nn.Dropout()的参数\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, heads=16, dim_head=64, dropout=0.):\n",
    "        super(MSA, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    " \n",
    "        # 论文里面的Dh\n",
    "        self.Dh = dim_head ** -0.5\n",
    " \n",
    "        # self-attention里面的Wq，Wk和Wv矩阵\n",
    "        inner_dim = dim_head * heads\n",
    "        self.linear_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.linear_k = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.linear_v = nn.Linear(dim, inner_dim, bias=False)\n",
    " \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    " \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        :param input: 输入是embeding，[batch, N, D]\n",
    "        :return: MSA的计算结果的维度和输入维度是一样的\n",
    "        \"\"\"\n",
    "\n",
    "        # 首先计算q k v\n",
    "        # [batch, N, inner_dim]\n",
    "        q = self.linear_q(input)\n",
    "        k = self.linear_k(input)\n",
    "        v = self.linear_v(input)\n",
    "        print('input.shape = ',input.shape)\n",
    "        # 接着计算矩阵A\n",
    "        # [batch, N, N]\n",
    "        print(\"q.shape = \",q.shape)\n",
    "        print(\"k.shape = \",k.shape)\n",
    "        A = torch.mul(q,k.T) * self.Dh\n",
    "        A = torch.softmax(A.view(A.shape[0],-1), dim=-1)\n",
    "        A = A.view(A.shape[0], int(math.sqrt(A.shape[1])), int(math.sqrt(A.shape[1])))\n",
    " \n",
    "        # [batch, N, inner_dim]\n",
    "        SA = torch.bmm(A, v)\n",
    "        # [batch, N, D]\n",
    "        out = self.output(SA)\n",
    "        return out\n",
    " \n",
    " \n",
    " \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim=64):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.msa = MSA(dim)\n",
    "        self.mlp = MLP(dim, hidden_dim)\n",
    "    def forward(self, input):\n",
    "        output = self.norm(input)\n",
    "        output = self.msa(output)\n",
    "        output_s1 = output + input\n",
    "        output = self.norm(output_s1)\n",
    "        output = self.mlp(output)\n",
    "        output_s2 = output + output_s1\n",
    "        return output_s2\n",
    " \n",
    " \n",
    "class VIT(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim=64, num_classes=10, num_layers=10):\n",
    "        super(VIT, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(TransformerEncoder(dim, hidden_dim))\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95fce723-0c04-4cff-8e4d-3f2b25021f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4208903d-ce70-4624-968d-9b5245fcd7cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VisionTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-98d51598474a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVIT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvit1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVisionTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvit1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvit1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VisionTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "vit = VIT(28)\n",
    "vit1 = VisionTransformer(num_layers=2)\n",
    "if torch.cuda.is_available():\n",
    "    vit1 = vit1.cuda()\n",
    "\n",
    "# CrossEntropy = softmax + log + NLL loss\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(vit1.parameters(),lr=0.001)\n",
    "\n",
    "def train():\n",
    "    for data in data_loader_train:\n",
    "        x_train, y_train = data\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        y_hat = vit1(x_train)\n",
    "        loss = loss_func(y_hat, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss %.4f' % (epoch + 1, loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09096d77-1518-4724-a878-35cc1efb617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    for data in data_loader_test:\n",
    "        x_test, y_test = data\n",
    "        x_test = x_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "        outputs = vit1(x_test)\n",
    "        _, predict = torch.max(outputs.data, dim=1)\n",
    "        correct += torch.sum(predict == y_test.data)\n",
    "    correct = correct.detach().cpu().numpy()\n",
    "    print(\"correct rate = \",100 * correct/len(data_test))\n",
    "    return 100 * correct/len(data_test) *0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b45fe812-4e5c-4c8b-b861-afbef4af700a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ae1ec615a85e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0ml_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "lss = []\n",
    "acc = []\n",
    "epochs = []\n",
    "for epoch in range(10):\n",
    "    epochs.append(epoch+1)\n",
    "    ls = train();\n",
    "    ac = test();\n",
    "    l_tmp = ls.detach().cpu().numpy()\n",
    "    lss.append(l_tmp)\n",
    "    acc.append(ac);\n",
    "    \n",
    "plt.plot(epochs,acc,color='r',label='accuracy')  \n",
    "plt.xlabel('epochs')    \n",
    "plt.ylabel('y label')   \n",
    "plt.title(\"chart\")      \n",
    "plt.legend()   \n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs,lss,color=(0,0,0),label='loss')\n",
    "plt.xlabel('epochs')    \n",
    "plt.ylabel('y label')   \n",
    "plt.title(\"chart\")      \n",
    "plt.legend()            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8b3568-a9e9-4d07-bcdf-199218497717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PositionEmbs(nn.Module):\n",
    "    def __init__(self, num_patches, emb_dim, dropout_rate=0.1):\n",
    "        super(PositionEmbs, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n",
    "        if dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.pos_embedding\n",
    "\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MlpBlock(nn.Module):\n",
    "    \"\"\" Transformer Feed-Forward Block \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, mlp_dim, out_dim, dropout_rate=0.1):\n",
    "        super(MlpBlock, self).__init__()\n",
    "\n",
    "        # init layers\n",
    "        self.fc1 = nn.Linear(in_dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, out_dim)\n",
    "        # GELU激活函数\n",
    "        self.act = nn.GELU()\n",
    "        if dropout_rate > 0.0:\n",
    "            self.dropout1 = nn.Dropout(dropout_rate)\n",
    "            self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.dropout1 = None\n",
    "            self.dropout2 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.fc1(x)\n",
    "        out = self.act(out)\n",
    "        if self.dropout1:\n",
    "            out = self.dropout1(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LinearGeneral(nn.Module):\n",
    "    def __init__(self, in_dim=(768,), feat_dim=(12, 64)):\n",
    "        super(LinearGeneral, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(*in_dim, *feat_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(*feat_dim))\n",
    "\n",
    "    def forward(self, x, dims):\n",
    "        a = torch.tensordot(x, self.weight, dims=dims) + self.bias\n",
    "        return a\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim, heads=8, dropout_rate=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.head_dim = in_dim // heads\n",
    "        self.scale = self.head_dim ** 0.5\n",
    "\n",
    "        self.query = LinearGeneral((in_dim,), (self.heads, self.head_dim))\n",
    "        self.key = LinearGeneral((in_dim,), (self.heads, self.head_dim))\n",
    "        self.value = LinearGeneral((in_dim,), (self.heads, self.head_dim))\n",
    "        self.out = LinearGeneral((self.heads, self.head_dim), (in_dim,))\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        q = self.query(x, dims=([2], [0]))\n",
    "        k = self.key(x, dims=([2], [0]))\n",
    "        v = self.value(x, dims=([2], [0]))\n",
    "\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        out = out.permute(0, 2, 1, 3)\n",
    "\n",
    "        out = self.out(out, dims=([2, 3], [0, 1]))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_dim, mlp_dim, num_heads, dropout_rate=0.1, attn_dropout_rate=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.attn = SelfAttention(in_dim, heads=num_heads, dropout_rate=attn_dropout_rate)\n",
    "        if dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        self.norm2 = nn.LayerNorm(in_dim)\n",
    "        self.mlp = MlpBlock(in_dim, mlp_dim, in_dim, dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.norm1(x)\n",
    "        out = self.attn(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        out += residual\n",
    "        residual = out\n",
    "\n",
    "        out = self.norm2(out)\n",
    "        out = self.mlp(out)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_patches, emb_dim, mlp_dim, num_layers=12, num_heads=12, dropout_rate=0.1,\n",
    "                 attn_dropout_rate=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # positional embedding\n",
    "        self.pos_embedding = PositionEmbs(num_patches, emb_dim, dropout_rate)\n",
    "\n",
    "        # encoder blocks\n",
    "        in_dim = emb_dim\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            layer = EncoderBlock(in_dim, mlp_dim, num_heads, dropout_rate, attn_dropout_rate)\n",
    "            self.encoder_layers.append(layer)\n",
    "        self.norm = nn.LayerNorm(in_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.pos_embedding(x)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_size=(28, 28),\n",
    "                 patch_size=(4, 4),\n",
    "                 emb_dim=768,\n",
    "                 mlp_dim=3072,\n",
    "                 num_heads=12,\n",
    "                 num_layers=12,\n",
    "                 num_classes=10,\n",
    "                 attn_dropout_rate=0.0,\n",
    "                 dropout_rate=0.1,\n",
    "                 feat_dim=None):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        h, w = image_size\n",
    "\n",
    "        # embedding layer\n",
    "        fh, fw = patch_size\n",
    "        gh, gw = h // fh, w // fw\n",
    "        num_patches = gh * gw\n",
    "        self.embedding = nn.Conv2d(1, emb_dim, kernel_size=(fh, fw), stride=(fh, fw))\n",
    "        # class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "\n",
    "        # transformer\n",
    "        self.transformer = Encoder(\n",
    "            num_patches=num_patches,\n",
    "            emb_dim=emb_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            dropout_rate=dropout_rate,\n",
    "            attn_dropout_rate=attn_dropout_rate)\n",
    "\n",
    "        # classfier\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)  # (n, c, gh, gw)\n",
    "        emb = emb.permute(0, 2, 3, 1)  # (n, gh, hw, c)\n",
    "        b, h, w, c = emb.shape\n",
    "        emb = emb.reshape(b, h * w, c)\n",
    "\n",
    "        # prepend class token\n",
    "        cls_token = self.cls_token.repeat(b, 1, 1)\n",
    "        emb = torch.cat([cls_token, emb], dim=1)\n",
    "\n",
    "        # transformer\n",
    "        feat = self.transformer(emb)\n",
    "\n",
    "        # classifier\n",
    "        logits = self.classifier(feat[:, 0])\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7bde746-4ab2-42e1-ad5b-822232dbe875",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit1 = VisionTransformer(num_layers=2)\n",
    "#if torch.cuda.is_available():\n",
    " #   vit1 = vit1.cuda()\n",
    "\n",
    "# CrossEntropy = softmax + log + NLL loss\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(vit1.parameters(),lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02553467-33dd-4bfc-a975-1be27e10a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def train():\n",
    "    for data in data_loader_train:\n",
    "        x_train, y_train = data\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        y_hat = vit1(x_train)\n",
    "        loss = loss_func(y_hat, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss %.4f' % (epoch + 1, loss))\n",
    "    return loss\n",
    "'''\n",
    "def train1():\n",
    "    x_train, y_train = next(iter(data_loader_train))\n",
    "    #x_train = x_train.to(device)\n",
    "    #y_train = y_train.to(device)\n",
    "    y_hat = vit1(x_train)\n",
    "    loss = loss_func(y_hat, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch %d, loss %.4f' % (epoch + 1, loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a52e3aa-226e-4d04-aee3-6772c6442220",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def test():\n",
    "    correct = 0\n",
    "    for data in data_loader_test:\n",
    "        x_test, y_test = data\n",
    "        x_test = x_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "        outputs = vit1(x_test)\n",
    "        _, predict = torch.max(outputs.data, dim=1)\n",
    "        correct += torch.sum(predict == y_test.data)\n",
    "    correct = correct.detach().cpu().numpy()\n",
    "    print(\"correct rate = \",100 * correct/len(data_test))\n",
    "    return 100 * correct/len(data_test) *0.01'''\n",
    "\n",
    "def test1():\n",
    "    correct = 0\n",
    "    x_test, y_test = next(iter(data_loader_test))\n",
    "   # x_test = x_test.to(device)\n",
    "   # y_test = y_test.to(device)\n",
    "    outputs = vit1(x_test)\n",
    "    _, predict = torch.max(outputs.data, dim=1)\n",
    "    correct += torch.sum(predict == y_test.data)\n",
    "    correct = correct.detach().cpu().numpy()\n",
    "    print(\"correct rate = \",100 * correct/len(data_test))\n",
    "    return 100 * correct/len(data_test) *0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5337d-b5e2-4972-81ec-692ff9ff749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.3884\n",
      "correct rate =  0.09\n",
      "epoch 2, loss 2.6307\n",
      "correct rate =  0.18\n",
      "epoch 3, loss 2.6538\n",
      "correct rate =  0.09\n",
      "epoch 4, loss 2.8254\n",
      "correct rate =  0.11\n",
      "epoch 5, loss 2.4483\n",
      "correct rate =  0.25\n",
      "epoch 6, loss 2.2984\n",
      "correct rate =  0.22\n",
      "epoch 7, loss 2.5098\n",
      "correct rate =  0.19\n",
      "epoch 8, loss 2.3258\n",
      "correct rate =  0.3\n",
      "epoch 9, loss 1.8711\n",
      "correct rate =  0.26\n"
     ]
    }
   ],
   "source": [
    "lss = []\n",
    "acc = []\n",
    "epochs = []\n",
    "for epoch in range(10):\n",
    "    epochs.append(epoch+1)\n",
    "    ls = train1();\n",
    "    ac = test1();\n",
    "    l_tmp = ls.detach().cpu().numpy()\n",
    "    lss.append(l_tmp)\n",
    "    acc.append(ac);\n",
    "    \n",
    "plt.plot(epochs,acc,color='r',label='accuracy')  \n",
    "plt.xlabel('epochs')    \n",
    "plt.ylabel('y label')   \n",
    "plt.title(\"chart\")      \n",
    "plt.legend()   \n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs,lss,color=(0,0,0),label='loss')\n",
    "plt.xlabel('epochs')    \n",
    "plt.ylabel('y label')   \n",
    "plt.title(\"chart\")      \n",
    "plt.legend()            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e06c6f-17e4-43d2-b8a4-271d13b584f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
